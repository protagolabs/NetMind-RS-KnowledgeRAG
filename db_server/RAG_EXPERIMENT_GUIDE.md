# KnowledgeRAG å®éªŒç³»ç»Ÿæ·±åº¦è§£æä¸RAGå®éªŒè®¾è®¡æŒ‡å—

## ğŸ¯ ç³»ç»Ÿæ¶æ„åŸç†

### ğŸ“Š ç³»ç»Ÿåˆ†å±‚æ¶æ„

KnowledgeRAG å®éªŒç³»ç»Ÿé‡‡ç”¨**åˆ†å±‚æ¶æ„**è®¾è®¡ï¼Œè®©æ‚¨å¯ä»¥ï¼š
- **éš”ç¦»ä¸åŒå®éªŒ**ï¼šæ¯ä¸ªå®éªŒæœ‰ç‹¬ç«‹çš„æ•°æ®ç¯å¢ƒ
- **å¿«é€Ÿåˆ‡æ¢å®éªŒ**ï¼šæ— éœ€é‡æ–°é…ç½®æ•°æ®åº“è¿æ¥
- **æ ‡å‡†åŒ–æµç¨‹**ï¼šç»Ÿä¸€çš„å®éªŒç®¡ç†å’Œæ•°æ®æ“ä½œæ¥å£

### ğŸ”§ æ ¸å¿ƒç»„ä»¶è§£æ

#### 1. **å®éªŒç®¡ç†å±‚**
- **experiment_manager.py**ï¼šç”¨æˆ·å‹å¥½çš„å®éªŒç®¡ç†ç•Œé¢
- **experiment_data.py**ï¼šåº•å±‚æ•°æ®ç®¡ç†å™¨ï¼Œæ”¯æŒå¤šæ•°æ®æº
- **experiment_schemas.py**ï¼šæ•°æ®åº“è¡¨ç»“æ„æ¨¡æ¿ç®¡ç†

#### 2. **æ•°æ®å­˜å‚¨å±‚**
- **MySQL**ï¼šå­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®ã€ç‰ˆæœ¬ä¿¡æ¯ã€æ–‡æœ¬å—ï¼ˆchunksï¼‰
- **Milvus**ï¼šå­˜å‚¨å‘é‡embeddingsï¼Œæ”¯æŒè¯­ä¹‰æœç´¢
- **æœ¬åœ°å¯¹è±¡å­˜å‚¨**ï¼šå­˜å‚¨åŸå§‹æ–‡ä»¶ï¼ˆPDFã€DOCã€å›¾ç‰‡ç­‰ï¼‰

#### 3. **åº”ç”¨å·¥å…·å±‚**
- **mysql_client.py**ï¼šMySQLæ“ä½œå°è£…
- **milvus_client.py**ï¼šMilvuså‘é‡æ“ä½œå°è£…
- **s3_local.py**ï¼šæœ¬åœ°æ–‡ä»¶å­˜å‚¨æ“ä½œ
- **flexible_search.py**ï¼šç»Ÿä¸€æœç´¢æ¥å£

---

## ğŸš€ å®éªŒç®¡ç†çš„æ ¸å¿ƒä»·å€¼

### ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦å®éªŒç®¡ç†ï¼Ÿ

#### 1. **æ•°æ®éš”ç¦»**
```bash
# ä¸åŒå®éªŒæœ‰ç‹¬ç«‹çš„æ•°æ®ç¯å¢ƒ
å®éªŒA: knowledge_rag_experiment_a
å®éªŒB: knowledge_rag_experiment_b
```

#### 2. **ç‰ˆæœ¬ç®¡ç†**
```bash
# åŒä¸€ä¸ªå®éªŒå¯ä»¥æœ‰å¤šä¸ªç‰ˆæœ¬
knowledge_rag_my_experiment/
â”œâ”€â”€ v1.0/  # ç¬¬ä¸€ç‰ˆå®éªŒ
â”œâ”€â”€ v1.1/  # æ”¹è¿›ç‰ˆæœ¬
â””â”€â”€ v2.0/  # é‡å¤§æ›´æ–°
```

#### 3. **å¿«é€Ÿåˆ‡æ¢**
```bash
# æ— éœ€é‡æ–°é…ç½®ï¼Œä¸€é”®åˆ‡æ¢å®éªŒç¯å¢ƒ
python experiment_manager.py --switch experiment_a
python experiment_manager.py --switch experiment_b
```

#### 4. **æ ‡å‡†åŒ–æµç¨‹**
- ç»Ÿä¸€çš„æ•°æ®è¡¨ç»“æ„
- æ ‡å‡†åŒ–çš„embeddingå­˜å‚¨
- ä¸€è‡´çš„æœç´¢æ¥å£

---

## ğŸ”¬ å®Œæ•´çš„RAGå®éªŒè®¾è®¡æµç¨‹

### ğŸ“‹ Phase 1: å®éªŒè§„åˆ’è®¾è®¡

#### 1.1 å®šä¹‰å®éªŒç›®æ ‡
```python
# ä¾‹ï¼šè®¾è®¡ä¸€ä¸ªå¤šæ–‡æ¡£é—®ç­”ç³»ç»Ÿ
å®éªŒåç§°: "multi_doc_qa_v1"
ç ”ç©¶ç›®æ ‡: æµ‹è¯•ä¸åŒembeddingæ¨¡å‹åœ¨å¤šæ–‡æ¡£æ£€ç´¢ä¸­çš„æ•ˆæœ
è¯„ä¼°æŒ‡æ ‡: å‡†ç¡®ç‡ã€å¬å›ç‡ã€å“åº”æ—¶é—´
```

#### 1.2 è®¾è®¡æ•°æ®ç»“æ„
```sql
-- è§„åˆ’éœ€è¦çš„è¡¨ç»“æ„
documents: æ–‡æ¡£åŸºæœ¬ä¿¡æ¯
document_versions: æ–‡æ¡£ç‰ˆæœ¬ç®¡ç†
chunks: æ–‡æœ¬åˆ†å—å­˜å‚¨
embeddings: å‘é‡ç´¢å¼•æ˜ å°„
search_logs: æœç´¢æ—¥å¿—è®°å½•
```

### ğŸ“‹ Phase 2: åˆ›å»ºå®éªŒç¯å¢ƒ

#### 2.1 åˆ›å»ºå®éªŒ
```bash
# æ–¹å¼1: äº¤äº’å¼åˆ›å»ºï¼ˆæ¨èï¼‰
python experiment_manager.py --interactive

# æ–¹å¼2: å‘½ä»¤è¡Œåˆ›å»º
python experiment_manager.py --create multi_doc_qa_v1 \
  --researcher "å¼ ä¸‰" \
  --description "å¤šæ–‡æ¡£é—®ç­”ç³»ç»Ÿå®éªŒ" \
  --template basic_rag
```

#### 2.2 éªŒè¯å®éªŒç¯å¢ƒ
```bash
# æ£€æŸ¥å®éªŒåˆ›å»ºç»“æœ
python experiment_manager.py --info multi_doc_qa_v1

# æ£€æŸ¥æ•°æ®å­˜å‚¨çŠ¶æ€
python experiment_data.py --action health-check
```

### ğŸ“‹ Phase 3: æ•°æ®ä¸Šä¼ ä¸å¤„ç†

#### 3.1 æ–‡æ¡£ä¸Šä¼ å®ç°
```python
# ä½¿ç”¨ s3_local.py ä¸Šä¼ æ–‡æ¡£
from knowledge_rag.utils.s3_local import S3LocalClient

s3_client = S3LocalClient()

# ä¸Šä¼ æ–‡æ¡£
def upload_document(user_id: int, doc_uuid: str, version: str, 
                   file_path: str, filename: str):
    """ä¸Šä¼ æ–‡æ¡£åˆ°æœ¬åœ°å¯¹è±¡å­˜å‚¨"""
    with open(file_path, 'rb') as file:
        uri = s3_client.put_object(
            user_id=user_id,
            doc_uuid=doc_uuid,
            version_label=version,
            filename=filename,
            file_stream=file,
            content_type="application/pdf"
        )
    return uri
```

#### 3.2 æ–‡æ¡£å…ƒæ•°æ®å­˜å‚¨
```python
# ä½¿ç”¨ mysql_client.py å­˜å‚¨å…ƒæ•°æ®
from knowledge_rag.utils.mysql_client import get_mysql_client

mysql_client = get_mysql_client()

# åˆ›å»ºæ–‡æ¡£è®°å½•
doc_id = mysql_client.create_document(
    user_id=1,
    title="æŠ€æœ¯æ–‡æ¡£.pdf",
    mime_type="application/pdf"
)

# åˆ›å»ºç‰ˆæœ¬è®°å½•
version_id = mysql_client.create_version(
    doc_id=doc_id,
    source_uri=uri,  # æ¥è‡ªS3ä¸Šä¼ çš„URI
    version_label="v1.0",
    checksum="sha256_hash_value"
)
```

#### 3.3 æ–‡æœ¬åˆ†å—å¤„ç†
```python
# æ–‡æ¡£è§£æå’Œåˆ†å—
def process_document(doc_id: int, version_id: int, file_content: str):
    """å¤„ç†æ–‡æ¡£ï¼šè§£æã€åˆ†å—ã€å­˜å‚¨"""
    
    # 1. æ–‡æ¡£è§£æï¼ˆç¤ºä¾‹ï¼‰
    chunks = split_text_into_chunks(file_content, chunk_size=512)
    
    # 2. å­˜å‚¨chunksåˆ°MySQL
    chunk_records = []
    for i, chunk_text in enumerate(chunks):
        chunk_uid = str(uuid.uuid4())
        chunk_record = ChunkIn(
            seq_no=i,
            chunk_uid=chunk_uid,
            text=chunk_text,
            token_count=len(chunk_text.split())
        )
        chunk_records.append(chunk_record)
    
    # æ‰¹é‡æ’å…¥chunks
    mysql_client.create_chunks(version_id, chunk_records)
    
    return chunk_records
```

### ğŸ“‹ Phase 4: å‘é‡åŒ–ä¸ç´¢å¼•

#### 4.1 ç”ŸæˆEmbeddings
```python
# ä½¿ç”¨ milvus_client.py å­˜å‚¨å‘é‡
from knowledge_rag.utils.milvus_client import get_milvus_client
import numpy as np

milvus_client = get_milvus_client()

def generate_embeddings(chunk_records: List[ChunkIn], doc_uuid: str, version: str):
    """ç”Ÿæˆå¹¶å­˜å‚¨å‘é‡embeddings"""
    
    for chunk in chunk_records:
        # 1. ç”Ÿæˆembeddingå‘é‡ï¼ˆç¤ºä¾‹ä½¿ç”¨éšæœºå‘é‡ï¼‰
        # å®é™…åº”ç”¨ä¸­ä½¿ç”¨ OpenAIã€Sentence-BERT ç­‰æ¨¡å‹
        embedding_vector = np.random.rand(768).tolist()  # 768ç»´å‘é‡
        
        # 2. å­˜å‚¨åˆ°Milvus
        success = milvus_client.upsert_embedding(
            embedding_id=hash(chunk.chunk_uid),
            user_id=1,
            doc_uuid=doc_uuid,
            version_label=version,
            chunk_uid=chunk.chunk_uid,
            vector=embedding_vector
        )
        
        if success:
            print(f"âœ… å‘é‡å­˜å‚¨æˆåŠŸ: {chunk.chunk_uid}")
```

### ğŸ“‹ Phase 5: æœç´¢åŠŸèƒ½å®ç°

#### 5.1 è¯­ä¹‰æœç´¢å®ç°
```python
# ä½¿ç”¨ flexible_search.py è¿›è¡Œæœç´¢
from knowledge_rag.utils.flexible_search import FlexibleSearchEngine, SearchQuery

def semantic_search(query_text: str, experiment_name: str, top_k: int = 5):
    """æ‰§è¡Œè¯­ä¹‰æœç´¢"""
    
    # 1. åˆå§‹åŒ–æœç´¢å¼•æ“
    search_engine = FlexibleSearchEngine(experiment_name=experiment_name)
    
    # 2. æ„é€ æœç´¢æŸ¥è¯¢
    query = SearchQuery(
        query_text=query_text,
        query_type="semantic",
        top_k=top_k,
        threshold=0.7,
        experiment_name=experiment_name
    )
    
    # 3. æ‰§è¡Œæœç´¢
    results = search_engine.search(query)
    
    # 4. è¿”å›ç»“æœ
    return results
```

#### 5.2 æ··åˆæœç´¢å®ç°
```python
def hybrid_search(query_text: str, filters: Dict[str, Any], top_k: int = 5):
    """æ··åˆæœç´¢ï¼šå‘é‡æœç´¢ + å…³é”®è¯æœç´¢"""
    
    # 1. å‘é‡æœç´¢
    vector_results = semantic_search(query_text, "multi_doc_qa_v1", top_k)
    
    # 2. å…³é”®è¯æœç´¢
    keyword_query = SearchQuery(
        query_text=query_text,
        query_type="keyword",
        filters=filters,
        top_k=top_k
    )
    
    search_engine = FlexibleSearchEngine("multi_doc_qa_v1")
    keyword_results = search_engine.search(keyword_query)
    
    # 3. ç»“æœèåˆ
    merged_results = merge_search_results(vector_results, keyword_results)
    
    return merged_results
```

### ğŸ“‹ Phase 6: å®éªŒæµ‹è¯•ä¸è¯„ä¼°

#### 6.1 åˆ›å»ºæµ‹è¯•é›†
```python
# åˆ›å»ºè¯„ä¼°æµ‹è¯•é›†
test_cases = [
    {
        "question": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "expected_docs": ["doc1", "doc2"],
        "ground_truth": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯..."
    },
    {
        "question": "æ·±åº¦å­¦ä¹ çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
        "expected_docs": ["doc3"],
        "ground_truth": "æ·±åº¦å­¦ä¹ å…·æœ‰è‡ªåŠ¨ç‰¹å¾æå–èƒ½åŠ›..."
    }
]
```

#### 6.2 æ‰¹é‡æµ‹è¯•
```python
def run_experiment_test(test_cases: List[Dict], experiment_name: str):
    """è¿è¡Œå®éªŒæµ‹è¯•"""
    
    results = []
    
    for i, test_case in enumerate(test_cases):
        print(f"\nğŸ” æµ‹è¯•ç”¨ä¾‹ {i+1}: {test_case['question']}")
        
        # 1. æ‰§è¡Œæœç´¢
        search_results = semantic_search(
            query_text=test_case['question'],
            experiment_name=experiment_name,
            top_k=5
        )
        
        # 2. è¯„ä¼°ç»“æœ
        evaluation = evaluate_search_results(search_results, test_case)
        
        # 3. è®°å½•ç»“æœ
        results.append({
            'test_case': test_case,
            'search_results': search_results,
            'evaluation': evaluation
        })
        
        print(f"   å‡†ç¡®ç‡: {evaluation['accuracy']:.2f}")
        print(f"   å¬å›ç‡: {evaluation['recall']:.2f}")
    
    return results
```

#### 6.3 æ€§èƒ½è¯„ä¼°
```python
def evaluate_search_results(search_results: List[SearchResult], test_case: Dict):
    """è¯„ä¼°æœç´¢ç»“æœè´¨é‡"""
    
    # è®¡ç®—å‡†ç¡®ç‡
    retrieved_docs = [result.metadata.get('doc_uuid') for result in search_results]
    expected_docs = test_case['expected_docs']
    
    # è®¡ç®—æŒ‡æ ‡
    precision = len(set(retrieved_docs) & set(expected_docs)) / len(retrieved_docs)
    recall = len(set(retrieved_docs) & set(expected_docs)) / len(expected_docs)
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    return {
        'accuracy': precision,
        'recall': recall,
        'f1_score': f1_score,
        'retrieved_docs': retrieved_docs,
        'expected_docs': expected_docs
    }
```

---

## ğŸ¯ å®é™…æ“ä½œç¤ºä¾‹

### ğŸ“ å®Œæ•´çš„RAGå®éªŒå®ç°

```python
#!/usr/bin/env python3
"""
å®Œæ•´çš„RAGå®éªŒç¤ºä¾‹
å®éªŒåç§°: multi_doc_qa_v1
ç›®æ ‡: æµ‹è¯•å¤šæ–‡æ¡£é—®ç­”ç³»ç»Ÿçš„æ•ˆæœ
"""

import os
import json
from pathlib import Path
from typing import List, Dict, Any

# å¯¼å…¥å·¥å…·æ¨¡å—
from knowledge_rag.utils.s3_local import S3LocalClient
from knowledge_rag.utils.mysql_client import get_mysql_client
from knowledge_rag.utils.milvus_client import get_milvus_client
from knowledge_rag.utils.flexible_search import FlexibleSearchEngine, SearchQuery

class RAGExperiment:
    """RAGå®éªŒç±»"""
    
    def __init__(self, experiment_name: str):
        self.experiment_name = experiment_name
        self.s3_client = S3LocalClient()
        self.mysql_client = get_mysql_client()
        self.milvus_client = get_milvus_client()
        self.search_engine = FlexibleSearchEngine(experiment_name)
    
    def upload_and_process_document(self, file_path: str, user_id: int = 1):
        """ä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£"""
        
        # 1. ä¸Šä¼ æ–‡æ¡£
        filename = Path(file_path).name
        doc_uuid = str(uuid.uuid4())
        
        with open(file_path, 'rb') as file:
            uri = self.s3_client.put_object(
                user_id=user_id,
                doc_uuid=doc_uuid,
                version_label="v1.0",
                filename=filename,
                file_stream=file
            )
        
        # 2. åˆ›å»ºæ–‡æ¡£è®°å½•
        doc_id = self.mysql_client.create_document(
            user_id=user_id,
            title=filename,
            mime_type="application/pdf"
        )
        
        # 3. åˆ›å»ºç‰ˆæœ¬è®°å½•
        version_id = self.mysql_client.create_version(
            doc_id=doc_id,
            source_uri=uri,
            version_label="v1.0",
            checksum="sha256_hash"
        )
        
        # 4. å¤„ç†æ–‡æ¡£å†…å®¹
        content = self.extract_text_from_pdf(file_path)
        chunks = self.split_text_into_chunks(content)
        
        # 5. å­˜å‚¨chunkså’Œembeddings
        self.store_chunks_and_embeddings(version_id, chunks, doc_uuid)
        
        return doc_id, version_id
    
    def test_search_functionality(self, test_questions: List[str]):
        """æµ‹è¯•æœç´¢åŠŸèƒ½"""
        
        for question in test_questions:
            print(f"\nğŸ” æµ‹è¯•é—®é¢˜: {question}")
            
            # æ‰§è¡Œæœç´¢
            query = SearchQuery(
                query_text=question,
                query_type="semantic",
                top_k=3,
                experiment_name=self.experiment_name
            )
            
            results = self.search_engine.search(query)
            
            # æ˜¾ç¤ºç»“æœ
            for i, result in enumerate(results, 1):
                print(f"   {i}. å¾—åˆ†: {result.score:.3f}")
                print(f"      å†…å®¹: {result.content[:100]}...")
                print(f"      æ¥æº: {result.metadata.get('doc_uuid', 'unknown')}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # 1. åˆ›å»ºå®éªŒ
    experiment = RAGExperiment("multi_doc_qa_v1")
    
    # 2. ä¸Šä¼ æµ‹è¯•æ–‡æ¡£
    docs_folder = "./test_documents/"
    for doc_file in Path(docs_folder).glob("*.pdf"):
        experiment.upload_and_process_document(str(doc_file))
    
    # 3. æµ‹è¯•æœç´¢
    test_questions = [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "æ·±åº¦å­¦ä¹ çš„ä¸»è¦åº”ç”¨é¢†åŸŸæœ‰å“ªäº›ï¼Ÿ",
        "å¦‚ä½•è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Ÿ"
    ]
    
    experiment.test_search_functionality(test_questions)
```

---

## ğŸ¯ å®éªŒç®¡ç†æœ€ä½³å®è·µ

### ğŸ“Š 1. å®éªŒå‘½åè§„èŒƒ
```bash
# æ¨èå‘½åæ ¼å¼
{é¡¹ç›®å}_{å®éªŒç±»å‹}_{ç‰ˆæœ¬å·}
ä¾‹å¦‚ï¼š
- qa_system_v1
- search_engine_baseline
- embedding_comparison_v2
```

### ğŸ“Š 2. ç‰ˆæœ¬æ§åˆ¶ç­–ç•¥
```bash
# ç‰ˆæœ¬æ§åˆ¶å»ºè®®
v1.0  # åˆå§‹ç‰ˆæœ¬
v1.1  # å°å¹…æ”¹è¿›
v2.0  # é‡å¤§æ›´æ–°
```

### ğŸ“Š 3. å®éªŒæ•°æ®ç®¡ç†
```bash
# å®šæœŸå¤‡ä»½é‡è¦å®éªŒ
python experiment_data.py --action backup-exp --experiment important_experiment

# æ¸…ç†è¿‡æœŸå®éªŒ
python experiment_manager.py --delete old_experiment --force
```

---

## ğŸš€ è¿›é˜¶åŠŸèƒ½

### ğŸ”§ è‡ªå®šä¹‰è¡¨ç»“æ„
```python
# åœ¨ experiment_schemas.py ä¸­å®šä¹‰è‡ªå®šä¹‰æ¨¡æ¿
custom_schema = {
    "name": "advanced_rag",
    "tables": {
        "documents": {
            "fields": [
                {"name": "id", "type": "INT", "primary": True},
                {"name": "title", "type": "VARCHAR(255)"},
                {"name": "category", "type": "VARCHAR(100)"},
                {"name": "priority", "type": "INT"},
                {"name": "created_at", "type": "TIMESTAMP"}
            ]
        },
        "custom_embeddings": {
            "fields": [
                {"name": "id", "type": "INT", "primary": True},
                {"name": "doc_id", "type": "INT"},
                {"name": "embedding_model", "type": "VARCHAR(50)"},
                {"name": "vector_data", "type": "JSON"}
            ]
        }
    }
}
```

### ğŸ”§ å¤šæ¨¡å‹æ¯”è¾ƒå®éªŒ
```python
# æ¯”è¾ƒä¸åŒembeddingæ¨¡å‹çš„æ•ˆæœ
models = ["sentence-bert", "openai-ada", "custom-model"]

for model in models:
    experiment_name = f"embedding_comparison_{model}"
    # åˆ›å»ºå®éªŒå¹¶æµ‹è¯•
    run_embedding_experiment(model, experiment_name)
```

---

## ğŸ“‹ å¸¸è§é—®é¢˜è§£ç­”

### â“ Q: å®éªŒä¹‹é—´çš„æ•°æ®ä¼šäº’ç›¸å½±å“å—ï¼Ÿ
**A**: ä¸ä¼šã€‚æ¯ä¸ªå®éªŒæœ‰ç‹¬ç«‹çš„æ•°æ®åº“å’Œå­˜å‚¨ç›®å½•ï¼Œå®Œå…¨éš”ç¦»ã€‚

### â“ Q: å¦‚ä½•åœ¨å®éªŒä¹‹é—´å…±äº«æ•°æ®ï¼Ÿ
**A**: å¯ä»¥ä½¿ç”¨å¤‡ä»½/æ¢å¤åŠŸèƒ½ï¼Œæˆ–è€…è®¾è®¡å…±äº«æ•°æ®è¡¨ç»“æ„ã€‚

### â“ Q: å®éªŒåˆ é™¤åèƒ½æ¢å¤å—ï¼Ÿ
**A**: åˆ é™¤å‰å»ºè®®å…ˆå¤‡ä»½ã€‚åˆ é™¤æ“ä½œä¼šæ¸…ç†æ‰€æœ‰ç›¸å…³æ•°æ®ã€‚

### â“ Q: å¦‚ä½•ç›‘æ§å®éªŒæ€§èƒ½ï¼Ÿ
**A**: ä½¿ç”¨å¥åº·æ£€æŸ¥åŠŸèƒ½å’Œè‡ªå®šä¹‰æ—¥å¿—è®°å½•ã€‚

---

## ğŸ¯ æ€»ç»“

KnowledgeRAG å®éªŒç³»ç»Ÿä¸ºæ‚¨æä¾›äº†ï¼š

1. **å®Œæ•´çš„æ•°æ®ç®¡ç†**ï¼šMySQL + Milvus + æœ¬åœ°å­˜å‚¨
2. **çµæ´»çš„å®éªŒç¯å¢ƒ**ï¼šå¿«é€Ÿåˆ›å»ºã€åˆ‡æ¢ã€åˆ é™¤å®éªŒ
3. **æ ‡å‡†åŒ–çš„å·¥å…·é“¾**ï¼šç»Ÿä¸€çš„æ•°æ®æ“ä½œå’Œæœç´¢æ¥å£
4. **å¯æ‰©å±•çš„æ¶æ„**ï¼šæ”¯æŒè‡ªå®šä¹‰è¡¨ç»“æ„å’Œæœç´¢ç®—æ³•

é€šè¿‡è¿™ä¸ªç³»ç»Ÿï¼Œæ‚¨å¯ä»¥ï¼š
- å¿«é€ŸéªŒè¯RAGç®—æ³•æ•ˆæœ
- æ¯”è¾ƒä¸åŒæ¨¡å‹æ€§èƒ½
- ç®¡ç†å¤æ‚çš„å®éªŒæ•°æ®
- å¤ç°å’Œåˆ†äº«å®éªŒç»“æœ

**å¼€å§‹æ‚¨çš„RAGå®éªŒä¹‹æ—…å§ï¼** ğŸš€ 